{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol2PBXbJljLW"
   },
   "source": [
    "# Predict Rents for Munich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4M4tAo7ljLY"
   },
   "source": [
    "Task: Download a collection of immoscout24 data for rents in Germany from kaggle at https://www.kaggle.com/corrieaar/apartment-rental-offers-in-germany and reduce it to the city of Munich.\n",
    "Examine which characteristics have a measurable influence on the rent.\n",
    "Create a prediction model that predicts the required rent as accurately as possible for given data.\n",
    "Also create visualizations of interesting relationships.\n",
    "Compare different methods for the inclusion of non-metric features and different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScnAiCGEljLZ"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wA4MYzUuljLZ"
   },
   "source": [
    "### Global imports and setings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQyEGZBYljLa"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5TOkXualjLa",
    "outputId": "46167b4c-1c90-4dff-b733-ddac0b433840"
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "!pip install pandas_profiling\n",
    "import pandas_profiling\n",
    "!pip install sklearn_pandas\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# additional regressors\n",
    "!pip install xgboost\n",
    "from xgboost import XGBRegressor\n",
    "!pip install lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# prediction application\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import TwoByTwoLayout\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgIPNuSaljLc"
   },
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"regession\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vTsA17xljLc"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRY-zPP9ljLd"
   },
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03lglh6MljLd"
   },
   "outputs": [],
   "source": [
    "# extract data from archive if not done already\n",
    "if not os.path.isfile('immo_data.csv'):\n",
    "    import zipfile\n",
    "    zipfile.ZipFile('apartment-rental-offers-in-germany.zip', 'r').extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMi5l4WXljLe"
   },
   "outputs": [],
   "source": [
    "# load/read data\n",
    "df = pd.read_csv(\"immo_data.csv\")\n",
    "df_origin = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BMG8CgnljLe",
    "outputId": "a744642d-be45-46ae-b5ec-b018f08ab05a"
   },
   "outputs": [],
   "source": [
    "print(\"appartements: %d\" % len(df.values))\n",
    "print(\"attributes: %d\" % len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UznNsvuZljLe"
   },
   "outputs": [],
   "source": [
    "report_enabled = True\n",
    "report = lambda df : report_enabled and df.reset_index(drop=True).profile_report(title='Team Munich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfq568wYljLf",
    "outputId": "37080d36-85d9-4ffe-aeb0-8568dfab0c65"
   },
   "outputs": [],
   "source": [
    "# display all avalible data columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3PZikI3ljLf"
   },
   "outputs": [],
   "source": [
    "# munich only\n",
    "df = df[df.regio2=='München']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dql9f23ljLg",
    "outputId": "51788ea3-c58e-43d0-ff59-878370a73b4d"
   },
   "outputs": [],
   "source": [
    "# show head\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpWsxMNGljLg",
    "outputId": "682edb5b-f040-4c8e-a91f-035c2ff8dade"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpE1qaj-ljLg"
   },
   "source": [
    "#### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmPv2HznljLh"
   },
   "source": [
    "Let first detect the difference between `totalRent` and `baseRent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEvDU-XCljLh",
    "outputId": "75d08b9c-139e-4f34-dcf1-f34cf66de5ab"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x='totalRent', y='baseRent', data=df)\n",
    "plt.title('$totalRent$ vs $baseRent$')\n",
    "plt.xlabel('totalRent')\n",
    "plt.ylabel('baseRent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7aZPl8oljLh"
   },
   "source": [
    "As we can see in the plot, there is no relevant difference in behaviour between `totalRent` and `baseRent`. Therefore the behavior will be almost identically in predictions.\n",
    "\n",
    "We choose `totalRent` as the basis feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jp_DDn5zljLh"
   },
   "outputs": [],
   "source": [
    "# if totalRent is not set use baseRent\n",
    "df['totalRent'][df['totalRent'] <= 0] = np.NaN\n",
    "df['totalRent'][np.isnan(df['totalRent'])] = (\n",
    "    df['baseRent'][np.isnan(df['totalRent'])]\n",
    "    + df['heatingCosts'][np.isnan(df['totalRent'])]\n",
    "    + df['serviceCharge'][np.isnan(df['totalRent'])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qioFYpNbljLi"
   },
   "outputs": [],
   "source": [
    "# analyze feature importance\n",
    "corr_matrix = df.corr()\n",
    "#corr_matrix['totalRent'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtJjiKymljLi",
    "outputId": "4121f0e7-a4e0-44c9-d2af-bde44471380d"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(corr_matrix[['totalRent']].sort_values(by='totalRent', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Features Correlating with totalRent', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1hlEwNnljLi"
   },
   "source": [
    "The important features for the `totalRent` prediction are:\n",
    "\n",
    "- baseRent\n",
    "- livingSpaceRange\n",
    "- baseRentRange\n",
    "- noRooms\n",
    "- noRoomsRange\n",
    "- serviceCharge\n",
    "- heatingCosts\n",
    "\n",
    "**Note**: It is important to know that the `corr_matrix`, or maybe it's more likely to say the correlation function of the pandas data frame, excludes values with `NaN` and `null`.\n",
    "\n",
    "> Compute pairwise correlation of columns, excluding NA/null values.\n",
    "\n",
    "Source: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "\n",
    "But as we already know, the `baseRent` is almost identical to `totalRent`. Therefore we also drop this feature. If we do not drop `baseRent` the prediction would based on itself which would falsify the result. Because `baseRentRange` is highly correlated with `baseRent`, we will drop `baseRentRange` as well.\n",
    "\n",
    "In other words if you keep y in X the prediction will only base on the feature y.\n",
    "\n",
    "Instead of the `-Range` features, we want to take the base features of them because we don't know how the `-Range` features were calculated.\n",
    "For this we need to confirm that `-Range` and base features are correlating with each others.\n",
    "\n",
    "Hence, this are the remaining features:\n",
    "\n",
    "- livingSpace\n",
    "- livingSpaceRange\n",
    "- noRooms\n",
    "- noRoomsRange\n",
    "- serviceCharge\n",
    "- heatingCosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTFQsiVEljLi"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['baseRent', 'baseRentRange'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWGpsK-8ljLj"
   },
   "source": [
    "To measure the association between the features and `totalRent` without using one-hot encode category features we will use Cramers V statistic calculation. For example we do not need to use one-hot encoding for `geo_plz`.\n",
    "\n",
    "More inforamtion about [Cramers V](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V) could be found in it's Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBq1diY9ljLj"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"\n",
    "    Cramers V statistic to calculate the measure of association\n",
    "    without one-hot encoding the category variable.\n",
    "    \"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-te3ZciCljLj"
   },
   "outputs": [],
   "source": [
    "importance = {}\n",
    "for feature in df.columns:\n",
    "    importance[feature] = cramers_v(df[feature], df['totalRent'])\n",
    "\n",
    "#pd.DataFrame(importance, index=[0]).T.sort_values(by=0, ascending=False) #.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs8G-UtUljLj",
    "outputId": "d9c54fd5-4b7d-4e98-ad05-030903afd715"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(pd.DataFrame(importance, index=[0]).T.sort_values(by=0, ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Cramers V: Features Importance', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjud-aVYljLj"
   },
   "source": [
    "Even Cramers V statistic provides that `geo_plz` does not associate with `totalRent`. Cramers V statistic also does not show any other suprises regarding the correlate with `totalRent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItYVp6b3ljLk"
   },
   "source": [
    "To examine the feature importance in more detail, we decided to also look at the Random Forest additionally to the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMBtl9i2ljLk"
   },
   "outputs": [],
   "source": [
    "# copy data set\n",
    "X_forest = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8icHTUDvljLk",
    "outputId": "60a74fcf-f308-4f8a-b302-758a4930688f"
   },
   "outputs": [],
   "source": [
    "# remove NaN values for feature important test\n",
    "X_forest = X_forest.replace('N/A',np.NaN)\n",
    "X_forest = X_forest.replace(np.NaN, 0)\n",
    "X_forest.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCibWnHPljLk"
   },
   "source": [
    "Hence `RandomForestRegressor` can not operate well on strings, we will use a `LabelEncoder` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uooy6AbgljLk",
    "outputId": "7b70e6c1-cd1d-40f0-b5b7-9fe584778209"
   },
   "outputs": [],
   "source": [
    "# get inoperable columns\n",
    "cols = X_forest.select_dtypes(include='object').columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8CFomU4ljLl",
    "outputId": "db393067-d0ac-4604-e008-2f62e2c283e9"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "les = {}\n",
    "\n",
    "original_tmp = X_forest\n",
    "mask = X_forest.isnull()  # NaN\n",
    "\n",
    "for col in cols:\n",
    "    X_forest[col] = X_forest[col].astype(str)\n",
    "    les[col] = LabelEncoder()\n",
    "    X_forest[col] = les[col].fit_transform(X_forest[col])\n",
    "\n",
    "X_forest = X_forest.where(~mask, original_tmp)\n",
    "X_forest[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTMD4PxtljLl"
   },
   "outputs": [],
   "source": [
    "# to inverse the Label-Encoding\n",
    "def le_inverse(col, df):\n",
    "    if (isinstance(col, list)):\n",
    "        return le_inverse_arr(col, df)\n",
    "\n",
    "    if not col in les:\n",
    "        return df[col]\n",
    "\n",
    "    return les[col].inverse_transform(df[col])\n",
    "\n",
    "def le_inverse_arr(cols, df):\n",
    "    res = []\n",
    "    for col in cols:\n",
    "        res.append(le_inverse(col, df))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87VR0EklljLq",
    "outputId": "9584bac9-2567-46cd-d068-95bc4f87f8c7"
   },
   "outputs": [],
   "source": [
    "le_inverse('heatingType', X_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akFhFuw5ljLr"
   },
   "source": [
    "Since we just used Label-Encoding, we can take a short look at feature selection with `SelectKBest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVgyA8xYljLr",
    "outputId": "8591f566-40c6-4aaa-d2a6-fad675118fb0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "X = X_forest.drop('totalRent', axis= 1)\n",
    "y = X_forest['totalRent']\n",
    "\n",
    "columns = []\n",
    "\n",
    "for k in range(1, len(X.columns)+1):\n",
    "    fiiiit = SelectKBest(f_regression, k=k).fit(X, y)\n",
    "    X_new = fiiiit.transform(X)\n",
    "    fiiiit.get_support()\n",
    "\n",
    "    cur_columns = X.columns[fiiiit.get_support()]\n",
    "\n",
    "    print(f\"{k:2}. {list(set(cur_columns)-set(columns))[0]}\")\n",
    "    columns = cur_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSYBGCpwljLr"
   },
   "source": [
    "No surprises so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jbb4ZLxljLr"
   },
   "source": [
    "Now we are prepared for the `RandomForestRegressor` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giWSuJP9ljLs"
   },
   "outputs": [],
   "source": [
    "# split y and X\n",
    "y_forest = X_forest['totalRent']\n",
    "X_forest = X_forest.drop('totalRent', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JYWOyrvljLs",
    "outputId": "3c1b28f1-8832-4b00-a8d6-3cdea089fed5"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "forest = forest.fit(X_forest, y_forest)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PPLdIpgljLs",
    "outputId": "ba717828-7661-44d0-bc63-07b980a3e01b"
   },
   "outputs": [],
   "source": [
    "# print the feature ranking\n",
    "print(\"Feature ranking\")\n",
    "print(\"===============\")\n",
    "for f in range(X_forest.shape[1]):\n",
    "    print(f\"{(f+1):2}.  {X_forest.columns[indices[f]]:<30} {importances[indices[f]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUQH7xiQljLy",
    "outputId": "8be972a8-8870-433e-b02e-07cfa744f6a0"
   },
   "outputs": [],
   "source": [
    "# Plot imporatance (Top 10)\n",
    "feat_importances = pd.Series(importances, index=X_forest.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh', title='Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Xs7KothljLy"
   },
   "source": [
    "Hence that the feature `geo_plz` and `picturecount` are important in the last plots, we decide to keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDa0hd9OljLy",
    "outputId": "bc84c532-3189-4a9c-c4c5-8ace149fa00a"
   },
   "outputs": [],
   "source": [
    "# check skewness of features\n",
    "skewness_for_features = [df[feature].skew(skipna=True)\n",
    "                         for feature in df.select_dtypes(exclude='object').columns]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.bar(df.select_dtypes(exclude='object').columns, skewness_for_features, figure=fig)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Skewness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5_eVh0FljLz"
   },
   "outputs": [],
   "source": [
    "df_skeweness = df.copy()\n",
    "df_skeweness['noParkSpaces']  = np.log(df_skeweness['noParkSpaces'])\n",
    "df_skeweness['livingSpace']   = np.log(df_skeweness['livingSpace'])\n",
    "df_skeweness['lastRefurbish'] = np.log(df_skeweness['lastRefurbish'])\n",
    "df_skeweness['geo_plz']       = np.log(df_skeweness['geo_plz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irVMVlOnljLz",
    "outputId": "1c93ee87-204d-4d7c-e5bf-becb2bbecc9e"
   },
   "outputs": [],
   "source": [
    "corr_matrix = df_skeweness[['noParkSpaces', 'livingSpace', 'geo_plz', 'lastRefurbish', 'totalRent']].corr()\n",
    "corr_matrix['totalRent'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sggAZvxljLz"
   },
   "source": [
    "As we can see, there is a high correlation between `livingSpace` and `totalRent`.\n",
    "So we take `livingSpace` as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1ALeWJBljLz"
   },
   "source": [
    "Lets have a look if `noParkSpaces` is also a good feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpsfhUtEljL0",
    "outputId": "8cc9d281-47b5-4f00-9cb4-003e8f56d48d"
   },
   "outputs": [],
   "source": [
    "df['noParkSpaces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0Fq_bazljL0",
    "outputId": "7ba6edbc-75ee-4f19-c408-5ebacdfaefe8"
   },
   "outputs": [],
   "source": [
    "# count NaN values\n",
    "len(df['noParkSpaces'][np.isnan(df['noParkSpaces'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYvPO1FeljL0"
   },
   "source": [
    "The feature`noParkSpaces` is ~44.2% not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNPfmk_0ljL0",
    "outputId": "c8d0c53e-88f9-430c-9f28-553e792b88eb"
   },
   "outputs": [],
   "source": [
    "# show unique types\n",
    "set(df['noParkSpaces'][~np.isnan(df['noParkSpaces'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swng2MaDljL1",
    "outputId": "364cb372-2031-49c4-9b99-a5d412ca62db"
   },
   "outputs": [],
   "source": [
    "len(df['noParkSpaces'][df['noParkSpaces'] > 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmX3OnBgljL1"
   },
   "source": [
    "There are seven appartements which have an unrealistic number of parking spaces.\n",
    "We decide to remove them from our dataset.\n",
    "\n",
    "Furthermore, we decide to set all appartments which doesn't set the `noParkSpaces` to zero, because by default an appartement in Munich doesn't provide an own parking space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mut7N0TEljL1"
   },
   "outputs": [],
   "source": [
    "df = df.drop(df['noParkSpaces'][df['noParkSpaces'] > 4].index)\n",
    "df['noParkSpaces'] = df['noParkSpaces'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uloduxHuljL1",
    "outputId": "04d161ef-2d6c-48df-df69-6fd9375f4f0a"
   },
   "outputs": [],
   "source": [
    "df['noParkSpaces'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY8VvAfgljL1"
   },
   "source": [
    "Now to check if we should add `noParkSpaces` lets check the correlation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPF-C3xyljL2",
    "outputId": "3eb2adf1-0b16-48d9-90ac-cdfa8fa7afb9"
   },
   "outputs": [],
   "source": [
    "# remove skeweness\n",
    "df['noParkSpaces'] = np.log(df['noParkSpaces'])\n",
    "\n",
    "# show correlation\n",
    "corr_matrix = df[['noParkSpaces', 'totalRent']].corr()\n",
    "corr_matrix['totalRent'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_xHBp7PljL2"
   },
   "source": [
    "We will add `noParkSpaces` as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApN9smGSljL9"
   },
   "outputs": [],
   "source": [
    "# revert skeweness\n",
    "df['noParkSpaces'] = np.exp(df['noParkSpaces'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aN-mszVAljL9"
   },
   "outputs": [],
   "source": [
    "# remaining features\n",
    "features = [\n",
    "    'totalRent',   # use this feature as y later\n",
    "\n",
    "    'livingSpace',\n",
    "    'livingSpaceRange',\n",
    "    'noRooms',\n",
    "    'noRoomsRange',\n",
    "    'serviceCharge',\n",
    "    'noParkSpaces',\n",
    "\n",
    "    'geo_plz',\n",
    "    'picturecount',\n",
    "\n",
    "    # for feature engeneering testing\n",
    "    #'heatingCosts',\n",
    "    #'condition',\n",
    "    #'heatingType',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz63zg7FljL-"
   },
   "source": [
    "Now we will drop all features which we do operate with anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bB7zYePljL-"
   },
   "outputs": [],
   "source": [
    "# drop all other features\n",
    "df = df.drop(df.columns.difference(features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e3076f43464e48bc96a8a4f3469d75dd",
      "e55e1eef9768490da48e97c2d66046be",
      "6d3525032eb04deb89c67a66e49cf80c"
     ]
    },
    "id": "8iDrUMQWljL-",
    "outputId": "a028ea06-ad42-4d6f-c9e4-2a04d8e62421"
   },
   "outputs": [],
   "source": [
    "df_plot = df.copy()\n",
    "df_plot['livingSpace'] = np.log(df_plot['livingSpace'])  # remove skeweness\n",
    "report(df_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfC3_NDdljL_"
   },
   "source": [
    "Out of the report we can see that `-Range` features are correlated with the base features\n",
    "\n",
    "Therefore, we are using only the base features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WR6aQhqbljL_",
    "outputId": "1a79b138-8dc6-4bd8-b3bc-434cf7f50962"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['livingSpaceRange', 'noRoomsRange'], axis=1)\n",
    "features.remove('livingSpaceRange')\n",
    "features.remove('noRoomsRange')\n",
    "print(features)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_htiT1eljMA"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqfIdKHGljMA"
   },
   "source": [
    "To better understand what the features mean, we could read their descripion from the source.\n",
    "\n",
    "features|description\n",
    "--|--\n",
    "totalRent         | total rent (usually a sum of base rent, service charge and heating cost)\n",
    "livingSpace       | living space in sqm\n",
    "noRooms           | number of rooms\n",
    "serviceCharge     | aucilliary costs such as electricty or internet in €\n",
    "noParkSpaces      | number of parking spaces\n",
    "gep_plz           | ZIP code\n",
    "picturecount      | how many pictures were uploaded to the listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6ib9IFDljMA",
    "outputId": "aceb4a85-3fb1-4bf5-b81b-6863e35619f0"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "70e7bb1d3c8246feab01d0e0fab75c40",
      "ffd2ff30dd0e4946a5d19bb0b1e55370",
      "73ccc6e4aa764079a5624a2f6410a7f2"
     ]
    },
    "id": "nJrx0ubEljMB",
    "outputId": "9dc7766e-aa03-49cc-b1dd-391aed727d61"
   },
   "outputs": [],
   "source": [
    "report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66JPVDrAljMB"
   },
   "source": [
    "If we look for example at the `serviceCharge` feature in the report, we see there are some appartements in the dataset which does not provide us a service charge value. Naturally, this could also be the case for other features.\n",
    "\n",
    "Values which are processable but not applicable are the most dangerous ones. We need to choose the best values or even remove these appartements from our prediction dataset.\n",
    "\n",
    "Special attention must be paid to the values `NaN`, `null` and `0`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2KWa5qiljMB"
   },
   "source": [
    "In the most cases there are three default approches how to replace `NaN` and invalid values.\n",
    "\n",
    "Here an example based on `serviceCharge`:\n",
    "\n",
    "- We could assume that no additional ancilliary costs such as electricty or internet are incurred in these cases. => `0`\n",
    "- We could choose the average/mean.\n",
    "- We could choose the median.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mLAp93xljMB"
   },
   "source": [
    "\n",
    "For `serviceCharge` we choose to set all Nan and negative values to the mean of all serviceCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eKbm5kfljMB"
   },
   "outputs": [],
   "source": [
    "## serviceCharge\n",
    "\n",
    "# set NaN and negative values to zero\n",
    "df['serviceCharge'][df['serviceCharge'] < 0] = 0\n",
    "df['serviceCharge'][np.isnan(df['serviceCharge'])] = np.mean(df['serviceCharge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZRq_prpljMB"
   },
   "outputs": [],
   "source": [
    "## noRooms\n",
    "\n",
    "# set invalid values to NaN\n",
    "df['noRooms'][df['noRooms'] < 1] = np.NaN\n",
    "df['noRooms'][df['noRooms'] > 1000] = np.NaN\n",
    "\n",
    "# calculate mean of valid values\n",
    "mean = int(np.mean(df['noRooms'][np.isnan(df['noRooms']) == False]))\n",
    "\n",
    "# set NaN to median\n",
    "df['noRooms'][np.isnan(df['noRooms'])] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYVp12IlljMC"
   },
   "outputs": [],
   "source": [
    "## heatingCosts\n",
    "\n",
    "# set NaN and negative values to zero\n",
    "#df['heatingCosts'][np.isnan(df['heatingCosts'])] = 0\n",
    "#df['heatingCosts'][df['heatingCosts'] < 0] = 0\n",
    "\n",
    "# calculate mean of none zero values\n",
    "#mean = np.mean(df['heatingCosts'][df['heatingCosts'] != 0])\n",
    "\n",
    "# set zero values to mean\n",
    "#df['heatingCosts'][df['heatingCosts'] == 0] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc12lx6SljMC"
   },
   "outputs": [],
   "source": [
    "## livingSpace\n",
    "\n",
    "# remove outliner\n",
    "df = df.drop(df['livingSpace'][df['livingSpace'] > 1000].index)\n",
    "\n",
    "# skeweness\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#livingSpace_scaler = MinMaxScaler()\n",
    "#df['livingSpace'] = livingSpace_scaler.fit_transform(df['livingSpace'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57QDfPpXljMC"
   },
   "source": [
    "Note: We have nothing to do for `noParkSpaces` regarding invalid large parking spaces anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6intB3UljMC"
   },
   "outputs": [],
   "source": [
    "## noParkSpaces\n",
    "\n",
    "# set NaN to invalid appatements\n",
    "df['noParkSpaces'][df['noParkSpaces'] < 0] = np.NaN\n",
    "\n",
    "# calculate mean\n",
    "mean = int(np.mean(df['noParkSpaces'][np.isnan(df['noParkSpaces']) == False]))\n",
    "\n",
    "# set invalid values to mean\n",
    "df['noParkSpaces'][np.isnan(df['noParkSpaces'])] = mean\n",
    "\n",
    "# skeweness\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#noParkSpaces_scaler = MinMaxScaler()\n",
    "#df['noParkSpaces'] = noParkSpaces_scaler.fit_transform(df['noParkSpaces'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IcTqyw6ljMC"
   },
   "outputs": [],
   "source": [
    "## geo_plz\n",
    "\n",
    "# drop all invalid post codes in Munich.\n",
    "df = df.drop(df['geo_plz'][df['geo_plz'] < 79999].index)\n",
    "df = df.drop(df['geo_plz'][df['geo_plz'] > 82000].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wAKJKdqljMC"
   },
   "outputs": [],
   "source": [
    "## totalRent\n",
    "\n",
    "# remove NaN\n",
    "df = df[df['totalRent'].notna()]\n",
    "\n",
    "# remove outliner\n",
    "df = df.drop(df['totalRent'][df['totalRent'] > 5000].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rvy3iCcCljMC"
   },
   "source": [
    "We remove invalid appartments and remove duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iECoMUxeljMD"
   },
   "outputs": [],
   "source": [
    "## picturecount\n",
    "df['picturecount'] = df['picturecount'].fillna(int(df['picturecount'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL2bXY5cljMD"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)       # remove index\n",
    "df = df.drop_duplicates(keep=False)  # remove dupplicated rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abOX8YmZljMD"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRe8TqAxljMD"
   },
   "source": [
    "We try to create a new feature that is more correlated to `totalRent` by adding `serviceCharge` and `heatingCosts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buAWYz4NljMD"
   },
   "outputs": [],
   "source": [
    "#df['serviceChargeAndHeatingCosts'] = df['serviceCharge'] + df['heatingCosts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylY2zaQOljMD"
   },
   "outputs": [],
   "source": [
    "#corr_matrix = df[['serviceChargeAndHeatingCosts', 'serviceCharge', 'heatingCosts', 'totalRent']].corr()\n",
    "#corr_matrix['totalRent'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2NCEamxljMD"
   },
   "source": [
    "```\n",
    "totalRent                       1.000000\n",
    "serviceChargeAndHeatingCosts    0.529311\n",
    "serviceCharge                   0.501365\n",
    "heatingCosts                    0.293588\n",
    "Name: totalRent, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-3Rf-pYljMD"
   },
   "outputs": [],
   "source": [
    "#df = df.drop(['serviceCharge', 'heatingCosts'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkZOCcnVljME"
   },
   "source": [
    "#### <span style=\"color: #FF8888\">**Unfortunately, feature engineering doesn't improved the overall prediction.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt9S2NWgljME"
   },
   "source": [
    "Even if we try to look for refurbished apartements or apartements with good heating we couldn't improve the prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKb5dnkOljME"
   },
   "outputs": [],
   "source": [
    "# make a single binary variable to indicate if the apartment is refurbished/new\n",
    "#df['refurbished'] = (df.condition == 'refurbished') | (df.condition == 'first_time_use') | \\\n",
    "#                    (df.condition == 'mint_condition') | (df.condition == 'fully_renovated') | \\\n",
    "#                    (df.condition == 'first_time_use_after_refurbishment')\n",
    "\n",
    "#df = df.drop(['condition'], axis=1)\n",
    "#df['refurbished']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9m4V0GKljME"
   },
   "outputs": [],
   "source": [
    "# make a binary variable to indicated if the rental property has good heating\n",
    "#df['goodHeating'] = (df.heatingType == 'central_heating') | (df.heatingType == 'floor_heating') | \\\n",
    "#                    (df.heatingType == 'self_contained_central_heating')\n",
    "\n",
    "#df = df.drop(['heatingType'], axis=1)\n",
    "#df['goodHeating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6eP9LszljME"
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUmBNpRfljME",
    "outputId": "2d9f337d-60ef-4150-d833-1cc16f45b7b2"
   },
   "outputs": [],
   "source": [
    "df.plot(subplots=True, layout=(3, 3), figsize=(12, 6), sharex=False, title=\"Feature Overview\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWHu3RozljME",
    "outputId": "57a2fb48-80f1-4c2f-d8c0-cd9104445eb9"
   },
   "outputs": [],
   "source": [
    "df.plot(subplots=True, figsize=(10, 30));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_wJ4u-rljMF"
   },
   "source": [
    "Let's look at the correlation matrix again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9TMYW8tljMF",
    "outputId": "ae16afa1-7fbc-4bff-9604-76dc0e654d62"
   },
   "outputs": [],
   "source": [
    "# analyze feature importance\n",
    "corr_matrix = df.corr()\n",
    "corr_matrix['totalRent'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "61bc2e12dbc14cd6acc6ca976636f529",
      "f054315526a644c1984b8655d275a37f",
      "8f408784525d46a5b442b1bdd5efeade"
     ]
    },
    "id": "KmtJDWmQljMF",
    "outputId": "10ecaed3-1bac-4788-da61-541fa76e145e"
   },
   "outputs": [],
   "source": [
    "report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvHq6oYhljMF"
   },
   "source": [
    "To save our clean data we export them as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpNPC52ZljMF"
   },
   "outputs": [],
   "source": [
    "# export the cleaned data\n",
    "df.to_csv(\"immo_data_clean.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJUEvxTXljMF"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXIV8CgSljMF"
   },
   "source": [
    "Load the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xxm4pOguljMF"
   },
   "outputs": [],
   "source": [
    "# load/read data\n",
    "df = pd.read_csv(\"immo_data_clean.csv\")\n",
    "df_origin = df.copy()  # yes override this | from now on origin means cleaned data\n",
    "\n",
    "# remaining features\n",
    "features = list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8r9q8vBljMF"
   },
   "source": [
    "Now let's split `totalRent` `y` from our dataset `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXGzQNnYljMG"
   },
   "outputs": [],
   "source": [
    "y = df['totalRent']\n",
    "X = df.drop('totalRent', axis=1)\n",
    "features.remove('totalRent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8Pb-Bj_ljMG"
   },
   "source": [
    "Furthermore we will now split and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs3y1kNBljMG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jP3SLdpzljMG"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale(X, y, X_train, y_train, X_test):\n",
    "    X_scaler, y_scaler = StandardScaler(), StandardScaler()\n",
    "\n",
    "    X_scaled = X_scaler.fit_transform(X)\n",
    "    y_scaled = y_scaler.fit_transform(\n",
    "        y.values.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        data=X_scaler.transform(X_train),\n",
    "        columns=X.columns\n",
    "    )\n",
    "    y_train_scaled = y_scaler.transform(\n",
    "        y_train.values.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        data=X_scaler.transform(X_test),\n",
    "        columns=X.columns\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        X_scaled, y_scaled,\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        X_test_scaled,\n",
    "        X_scaler, y_scaler\n",
    "    ]\n",
    "\n",
    "X_scaled, y_scaled, X_train_scaled, y_train_scaled, X_test_scaled, X_scaler, y_scaler = scale(X, y, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ai6M4s8ljMG"
   },
   "source": [
    "Let's define a little helper function to evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PNmWVq1ljMG"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from pprint import pprint\n",
    "\n",
    "def evaluate(model):\n",
    "    def round_decimal(x):\n",
    "        return f\"{(int((x*100) + 0.5) / 100.0):0.2f}\"\n",
    "\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    predictions = y_scaler.inverse_transform(predictions)\n",
    "\n",
    "    errors = abs(predictions - y_test)\n",
    "    mape = 100 * np.mean(errors / y_test)\n",
    "    accuracy = 100 - mape\n",
    "    mae = mean_absolute_error(y_test, predictions)  # np.mean(errors)\n",
    "    min_error = np.min(errors)\n",
    "    max_error = np.max(errors)\n",
    "\n",
    "    print('Model Performance')\n",
    "    print('=================')\n",
    "    print(f\"Mean Absolute Error : {round_decimal(mae):>10}€\")\n",
    "    print(f\"Min. Error          : {round_decimal(min_error):>10}€\")\n",
    "    print(f\"Max. Error          : {round_decimal(max_error):>10}€\")\n",
    "    print(f\"Accuracy            : {round_decimal(accuracy):>10}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAYZzZHAljMG"
   },
   "source": [
    "Now we will start to apply some regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvNPTBIgljMG"
   },
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAkTcQQKljMG"
   },
   "source": [
    "> `< 100k samples?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05if2Z0SljMH",
    "outputId": "9eedbc64-3a64-48d8-f5e8-af7368468c69"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2pMWkp0ljMH"
   },
   "source": [
    "Yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EZwgtkQljMH"
   },
   "source": [
    "> Few features are important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jmoyTesljMH",
    "outputId": "e056def4-f84a-4537-a0f5-7884020c6bce"
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNsOiBcVljMH"
   },
   "source": [
    "Yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1W8nvewljMH"
   },
   "source": [
    "That means that we start with ElasticNet and Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cS5oQZ7aljMH"
   },
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLyM7rjLljMH",
    "outputId": "a75560e1-61d0-4da7-fd9e-faef8fd7bbe4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "en_regr = ElasticNetCV(cv=5, random_state=42)\n",
    "en_regr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(en_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E80A6YfDljMI"
   },
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GVX6BC-ljMI",
    "outputId": "90b6c861-9adb-42af-d242-79492f8911a1"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "l_regr = LassoCV(cv=5, random_state=42)\n",
    "l_regr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(l_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcgmyWm5ljMI"
   },
   "source": [
    "Both models are working quite well. Before we start the hyperparameter tuning we want to try some additional models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnk2XPNGljMI"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2cq78i9ljMI"
   },
   "source": [
    "Let's check if an ensemble Regressor could add some impremovents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "991lJtowljMI",
    "outputId": "84355742-3f7b-40bc-dd89-9e755f51b6e0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr_regr = RandomForestRegressor(random_state=42)\n",
    "rfr_regr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(rfr_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPCE2P1IljMI"
   },
   "source": [
    "Despite the fewer features, Random Forest seems to reach the better performance.\n",
    "\n",
    "Hence we will try to optimize this regressor (hyperparameter-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkjCJ1XxljMI"
   },
   "source": [
    "### Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWNq_kiIljMJ",
    "outputId": "8ab85db1-ed9a-4ffd-e414-913f46b076ec"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(\n",
    "    rfr_regr,\n",
    "    X_train_scaled,\n",
    "    y_train_scaled,\n",
    "    cv=5,\n",
    "    scoring=('neg_mean_absolute_error'),\n",
    "    return_train_score=True,\n",
    "    return_estimator=True\n",
    ")\n",
    "#pprint(scores)\n",
    "\n",
    "min_index = np.argwhere(np.abs(scores['train_score']) == np.min(np.abs(scores['train_score'])))\n",
    "min_index = np.min(min_index)\n",
    "rfr_regr = scores['estimator'][min_index]\n",
    "\n",
    "evaluate(rfr_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln55wB_nljMJ"
   },
   "source": [
    "### RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgeDQ_AcljMJ",
    "outputId": "ba721d5c-c5de-456d-fbb8-5c941ae02538"
   },
   "outputs": [],
   "source": [
    "n_estimators      = [ int(x) for x in np.linspace(start=200, stop=2000, num=10) ]\n",
    "max_features      = [ 'auto', 'sqrt' ]\n",
    "max_depth         = [ None ] + [ int(x) for x in np.linspace(10, 110, num=11) ]\n",
    "min_samples_split = [ 2, 5, 10 ]\n",
    "min_samples_leaf  = [ 1, 2, 4 ]\n",
    "bootstrap         = [ True, False ]\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators'      : n_estimators,\n",
    "    'max_features'      : max_features,\n",
    "    'max_depth'         : max_depth,\n",
    "    'min_samples_split' : min_samples_split,\n",
    "    'min_samples_leaf'  : min_samples_leaf,\n",
    "    'bootstrap'         : bootstrap\n",
    "}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axzxxOTzljMJ",
    "outputId": "bb3a958f-e834-48e1-c421-7befd845ea2f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rfr_regr = RandomForestRegressor()\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rfr_regr,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "pprint(rf_random.best_params_)\n",
    "evaluate(rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkU6umQLljMJ"
   },
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GFsQvTiljMJ",
    "outputId": "527251e1-15a1-4b05-e0a4-03ff46668efd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap'         : [ True ],\n",
    "    'max_depth'         : [ 80, 90, 100, 110 ],\n",
    "    'max_features'      : [ 2, 3 ],\n",
    "    'min_samples_leaf'  : [ 3, 4, 5 ],\n",
    "    'min_samples_split' : [ 8, 10, 12 ],\n",
    "    'n_estimators'      : [ 100, 200, 300, 1000 ]\n",
    "}\n",
    "\n",
    "rfr_regr = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rfr_regr,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "pprint(grid_search.best_params_)\n",
    "evaluate(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnlbKDQYljMJ"
   },
   "source": [
    "With over 87% we already do have a well trained model. But let's see if we can still find a better one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgQrgfLBljMK"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQc8sEKbljMK",
    "outputId": "08470fc2-e353-464d-ae1b-f57c4e14a767"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_regr = GradientBoostingRegressor(random_state=42)\n",
    "gb_regr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(gb_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4eRc_rOljMK"
   },
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGNJC4vfljMK",
    "outputId": "01955655-84f4-41cf-a214-af79bb9f4b8c"
   },
   "outputs": [],
   "source": [
    "n_estimators      = [ int(x) for x in np.linspace(start=200, stop=2000, num=10) ]\n",
    "max_features      = [ 'auto', 'sqrt' ]\n",
    "max_depth         = [ None ] + [ int(x) for x in np.linspace(10, 110, num=11) ]\n",
    "min_samples_split = [ 2, 5, 10 ]\n",
    "min_samples_leaf  = [ 1, 2, 4 ]\n",
    "learning_rate     = [ 0.25, 0.2, 0.15, 0.1, 0.05, 0.01 ]\n",
    "\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators'      : n_estimators,\n",
    "    'max_features'      : max_features,\n",
    "    'max_depth'         : max_depth,\n",
    "    'min_samples_split' : min_samples_split,\n",
    "    'min_samples_leaf'  : min_samples_leaf,\n",
    "    'learning_rate'     : learning_rate\n",
    "}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4o-JCKWrljMK",
    "outputId": "27821daf-a26e-4225-b53b-d484d5b0c1cc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gb_regr = GradientBoostingRegressor()\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=gb_regr,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "pprint(rf_random.best_params_)\n",
    "evaluate(rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaixatI3ljMK"
   },
   "source": [
    "##  Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACV2i21iljMK",
    "outputId": "53ea92aa-bf68-4ea4-b367-415f16b9b04e"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_regr = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_regr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(sgd_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcU6CE_tljML"
   },
   "source": [
    "## Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IckuJhj_ljML",
    "outputId": "0be8afd9-451b-467d-db0a-d42efd6f3563"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "sv_regr = SVR(kernel = 'rbf')\n",
    "sv_regr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(sv_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zx49XJgOljML"
   },
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izE5fYf-ljML",
    "outputId": "2de02ebd-0516-46ff-dd5a-3e2e2dd7ac90"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'kernel' : [ 'linear', 'poly', 'rbf', 'sigmoid' ],\n",
    "    'degree' : [ 2, 3, 4, 5 ],\n",
    "    'C'      : [ 0.1, 0.5, 1.0, 2.0 ]\n",
    "}\n",
    "\n",
    "sv_regr = SVR()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=sv_regr,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "pprint(grid_search.best_params_)\n",
    "evaluate(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7hn5WqbljML"
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7axe5w4ljML",
    "outputId": "82df387a-cc44-4556-95c3-00a0db110353"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada_regr = AdaBoostRegressor(random_state=42, n_estimators=100)\n",
    "ada_regr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(ada_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vORNWpFljML"
   },
   "source": [
    "## Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7GovCDsljMM",
    "outputId": "ad85bb44-ec7c-4d7a-c23c-d1fe6f2eb88c"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGiVbT--ljMM"
   },
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2a2CaQjljMM",
    "outputId": "e3d399f1-a54e-4469-8605-5c59adf10ea9"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'nthread'          : [ 4 ],  # when use hyperthread, xgboost may become slower\n",
    "    'objective'        : [ 'reg:squarederror' ],\n",
    "    'learning_rate'    : [ 0.03, 0.05 ],  # so called `eta` value\n",
    "    'max_depth'        : [ 5, 6 ],\n",
    "    'min_child_weight' : [ 4 ],\n",
    "    'subsample'        : [ 0.7 ],\n",
    "    'colsample_bytree' : [ 0.7 ],\n",
    "    'n_estimators'     : [ 500, 1000 ]\n",
    "}\n",
    "\n",
    "xgb1 = XGBRegressor()\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb1,\n",
    "    parameters,\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "pprint(xgb_grid.best_params_)\n",
    "evaluate(xgb_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvvR1OREljMM"
   },
   "source": [
    "## Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIRc7oLfljMM",
    "outputId": "66b41999-7280-4623-9f89-fc4c876e1561"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "model = LGBMRegressor(random_state=42)\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvKCmkBdljMM",
    "outputId": "db1cb070-d909-4f0f-a596-d39adaa802dd"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "parameters = {\n",
    "    'num_iter'      : [ 50, 100,  500, 1000 ],\n",
    "    'learning_rate' : [ 0.25, 0.1, 0.05, 0.005 ],\n",
    "    'n_estimators'  : [ 25, 100, 1000, 2000 ],\n",
    "    'max_bin'       : [ 500, 1000 ],\n",
    "}\n",
    "\n",
    "model = LGBMRegressor(random_state=42)\n",
    "lgbm_grid = GridSearchCV(\n",
    "    model,\n",
    "    parameters,\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "lgbm_grid.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "pprint(lgbm_grid.best_params_)\n",
    "evaluate(lgbm_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTfwgSuYljMN"
   },
   "source": [
    "Light Gradient Boosting Machine not only work with an amazing speed but also has some nice build in features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8mnIC0PljMN",
    "outputId": "51804d6b-45c1-4558-dca1-040c89190e33"
   },
   "outputs": [],
   "source": [
    "from lightgbm import plot_importance\n",
    "plot_importance(booster=lgbm_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz48xEyNljMN"
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hFlWYhvljMN"
   },
   "source": [
    "| Model | MAE | Min | Max | Accuracy |\n",
    "|--|--:|--:|--:|--:|\n",
    "|ElasticNet|291.32€|0.30€|2066.67€|83.52%|\n",
    "|Lasso|291.24€|0.58€|2067.44€|83.52%|\n",
    "|Random Forest|236.47€|0.34€|1461.27€|86.80%|\n",
    "|Random Forest (CV)|237.88€|0.36€|1323.56€|86.67%|\n",
    "|Random Forest (RS)|232.06€|0.64€|1380.04€|87.05%|\n",
    "|Random Forest (GS)|233.54€|0.49€|1400.27€|87.01%|\n",
    "|Gradient Boosting|228.67€|0.31€|<span style=\"color: red\">**1234.59€**</span>|87.43%|\n",
    "|Gradient Boosting (RS)|229.08€|0.60€|1473.02€|87.30%|\n",
    "|Stochastic Gradient Descent|301.45€|0.48€|1854.29€|82.54%|\n",
    "|Support Vector|249.74€|0.06€|1759.94€|86.26%|\n",
    "|Support Vector (GS)|249.74€|0.06€|1759.94€|86.26%|\n",
    "|AdaBoost|329.29€|0.64€|1776.35€|79.99%|\n",
    "|Extreme Gradient Boosting|235.19€|0.72€|1474.05€|86.97%|\n",
    "|**Extreme Gradient Boosting (GS)**|<span style=\"color: red\">**220.39€**</span>|0.62€|1353.08€|<span style=\"color: red\">**87.82%**</span>|\n",
    "|Light Gradient Boosting Machine|224.46€|<span style=\"color: red\">**0.02€**</span>|1498.28€|87.69%|\n",
    "|Light Gradient Boosting Machine (GS)|229.48€|0.06€|1401.13€|87.35%|\n",
    "\n",
    "Legend:  \n",
    "CV: Cross Validation  \n",
    "RS: Randomized Search  \n",
    "GS: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNWu4QICljMO"
   },
   "source": [
    "There are a few well performing models. With a small lead Extreme Gradient Boosting Regressor is the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7t6vw_KljMO",
    "outputId": "5a6e8d03-3407-4678-c0ec-f808c7f3586a"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(\n",
    "    colsample_bytree=0.7,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    min_child_weight=4,\n",
    "    n_estimators=500,\n",
    "    nthread=4,\n",
    "    objective='reg:squarederror',\n",
    "    subsample=0.7,\n",
    ")\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2qMwSEHljMO"
   },
   "source": [
    "An interessing part could be to see the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUgq9xYLljMO",
    "outputId": "c58feade-84d2-4d5d-db84-40ca7a760160"
   },
   "outputs": [],
   "source": [
    "predictors=list(X_train)\n",
    "feat_imp = pd.Series(model.feature_importances_, predictors).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Importance of Features')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fijv2yHrljMO"
   },
   "source": [
    "## Prediction Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4OLvfUljMP"
   },
   "source": [
    "Now let's create a little prediction application for practical usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "59c4cc2315a04b7f85a7984c47854270",
      "2089d71071834f7ea62f1d89564c055a",
      "7d3ae40c53034807856f4f229c22a4d5",
      "500685ec36354335bec3362b7057bac8",
      "9ddb7095ef484ff690852e36c19f86c4",
      "4c8356e5c2434da8a147023aef7a1d4a",
      "7ff6e49751194e56b835d96d0976bcdc"
     ]
    },
    "id": "1x7tvoDSljMP",
    "outputId": "a70e90ad-b5e3-4b91-a53d-3342b013e3c0"
   },
   "outputs": [],
   "source": [
    "# style\n",
    "style = { 'description_width': '150px' }\n",
    "\n",
    "# living space (sqm)\n",
    "living_space = widgets.IntText(description='living space (sqm):', value=int(np.mean(df_origin['livingSpace'])), style=style)\n",
    "living_space2 = widgets.IntSlider(min=np.min(df_origin['livingSpace']), max=np.max(df_origin['livingSpace']), readout=False)\n",
    "widgets.jslink((living_space, 'value'), (living_space2, 'value'))\n",
    "living_space_grid = TwoByTwoLayout(top_left=living_space, top_right=living_space2, width=\"650px\")\n",
    "display(living_space_grid)\n",
    "\n",
    "# No. of rooms\n",
    "no_rooms = widgets.IntText(description='no. of rooms:', value=int(np.mean(df_origin['noRooms'])), style=style)\n",
    "no_rooms2 = widgets.IntSlider(min=np.min(df_origin['noRooms']), max=np.max(df_origin['noRooms']), readout=False)\n",
    "widgets.jslink((no_rooms, 'value'), (no_rooms2, 'value'))\n",
    "no_rooms_grid = TwoByTwoLayout(top_left=no_rooms, top_right=no_rooms2, width=\"650px\")\n",
    "display(no_rooms_grid)\n",
    "\n",
    "# service charge\n",
    "service_charge = widgets.FloatText(description='service charge:', value=int(np.mean(df_origin['serviceCharge'])*100)/100.0, style=style)\n",
    "service_charge2 = widgets.FloatSlider(min=np.min(df_origin['serviceCharge']), max=np.max(df_origin['serviceCharge']), readout=False)\n",
    "widgets.jslink((service_charge, 'value'), (service_charge2, 'value'))\n",
    "service_charge_grid = TwoByTwoLayout(top_left=service_charge, top_right=service_charge2, width=\"650px\")\n",
    "display(service_charge_grid)\n",
    "\n",
    "# picture count\n",
    "picturecount = widgets.IntSlider(\n",
    "    min=np.min(df_origin['picturecount']), max=np.max(df_origin['picturecount']),\n",
    "    value=int(np.mean(df_origin['picturecount'])), description='picture count:', style=style\n",
    ")\n",
    "\n",
    "# No. of parking spaces\n",
    "no_park_spaces = widgets.IntText(description='no. of parking spaces:', value=int(np.mean(df_origin['noParkSpaces'])), style=style)\n",
    "no_park_spaces2 = widgets.IntSlider(min=np.min(df_origin['noParkSpaces']), max=np.max(df_origin['noParkSpaces']), readout=False)\n",
    "widgets.jslink((no_park_spaces, 'value'), (no_park_spaces2, 'value'))\n",
    "no_park_spaces_grid = TwoByTwoLayout(top_left=no_park_spaces, top_right=no_park_spaces2, width=\"650px\")\n",
    "display(no_park_spaces_grid)\n",
    "\n",
    "# Post Code\n",
    "geo_plz = widgets.Dropdown(\n",
    "    options=sorted(set(df_origin['geo_plz'])),\n",
    "    value=Counter(df['geo_plz']).most_common(1)[0][0],\n",
    "    description='post code:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "display(geo_plz)\n",
    "\n",
    "# Go button\n",
    "def on_button_clicked(b):\n",
    "    data = {\n",
    "        'serviceCharge' : service_charge.value,\n",
    "        'picturecount'  : picturecount.value,\n",
    "        'noParkSpaces'  : no_park_spaces.value,\n",
    "        'livingSpace'   : living_space.value,\n",
    "        'geo_plz'       : geo_plz.value,\n",
    "        'noRooms'       : no_rooms.value,\n",
    "    }\n",
    "    data_scaled = pd.DataFrame(\n",
    "        data=X_scaler.transform([list(data.values())]),\n",
    "        columns=list(data.keys())\n",
    "    )\n",
    "    predict = y_scaler.inverse_transform(model.predict(data_scaled))[0]\n",
    "    prediction.value = f\"Result: {(int((predict*100) + 0.5) / 100.0):0.2f}€\"\n",
    "btnGo = widgets.Button(\n",
    "    description='Predict',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Start prediction!',\n",
    "    icon='legal',\n",
    ")\n",
    "btnGo.on_click(on_button_clicked)\n",
    "display(btnGo)\n",
    "# prediction label\n",
    "prediction = widgets.Label(value=\"Result: \")\n",
    "display(prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
